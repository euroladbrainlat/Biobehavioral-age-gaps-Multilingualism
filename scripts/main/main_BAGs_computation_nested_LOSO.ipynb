{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65176697-73f6-4266-abef-1325ee5a290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Caitlin Rivers\n",
    "Analysis functions for package epipy.\n",
    "\"\"\"\n",
    "def _get_table_labels(table):\n",
    "    \"\"\"\n",
    "    Returns classic a, b, c, d labels for contingency table calcs.\n",
    "    \"\"\"\n",
    "    a = table[0][0]\n",
    "    b = table[0][1]\n",
    "    c = table[1][0]\n",
    "    d = table[1][1]\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _ordered_table(table):\n",
    "    \"\"\"\n",
    "    Determine type of table input. Find classic a, b, c, d labels\n",
    "    for contigency table calculations.\n",
    "    \"\"\"\n",
    "    if type(table) is list:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    elif type(table) is pd.core.frame.DataFrame:\n",
    "        a, b, c, d = _get_table_labels(table.values)\n",
    "    elif type(table) is np.ndarray:\n",
    "        a, b, c, d = _get_table_labels(table)\n",
    "    else:\n",
    "        raise TypeError('table format not recognized')\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "def _conf_interval(ratio, std_error):\n",
    "    \"\"\"\n",
    "    Calculate 95% confidence interval for odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "\n",
    "    _lci = np.log(ratio) - 1.96*std_error\n",
    "    _uci = np.log(ratio) + 1.96*std_error\n",
    "\n",
    "    lci = round(np.exp(_lci), 2)\n",
    "    uci = round(np.exp(_uci), 2)\n",
    "\n",
    "    return (lci, uci)\n",
    "\n",
    "def _numeric_summary(column):\n",
    "    \"\"\"\n",
    "    Finds count, number of missing values, min, median, mean, std, and\n",
    "    max.\n",
    "    See summary()\n",
    "    \"\"\"\n",
    "    names = ['count', 'missing', 'min', 'median', 'mean', 'std', 'max']\n",
    "    _count = len(column)\n",
    "    _miss = _count - len(column.dropna())\n",
    "    _min = column.min()\n",
    "    _median = column.median()\n",
    "    _mean = column.mean()\n",
    "    _std = column.std()\n",
    "    _max = column.max()\n",
    "    summ = pd.Series([_count, _miss, _min, _median, _mean, _std, _max], index=names)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _categorical_summary(column, n=None):\n",
    "    \"\"\"\n",
    "    Finds count and frequency of each unique value in the column.\n",
    "    See summary().\n",
    "    \"\"\"\n",
    "    if n is not None:\n",
    "        _count = column.value_counts()[:n]\n",
    "    else:\n",
    "        _count = column.value_counts()\n",
    "    names = ['count', 'freq']\n",
    "    _freq = column.value_counts(normalize=True)[:n]\n",
    "    summ = pd.DataFrame([_count, _freq], index=names).T\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def _summary_calc(column, by=None):\n",
    "    \"\"\"\n",
    "    Calculates approporiate summary statistics based on data type.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    column = one column (series) of pandas df\n",
    "    by = optional. stratifies summary statistics by each value in the\n",
    "                column.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    if column data type is numeric, returns summary statistics\n",
    "    if column data type is an object, returns count and frequency of\n",
    "        top 5 most common values\n",
    "    \"\"\"\n",
    "    if column.dtype == 'float64' or column.dtype == 'int64':\n",
    "        coltype = 'numeric'\n",
    "    elif column.dtype == 'object':\n",
    "        coltype = 'object'\n",
    "\n",
    "\n",
    "    if by is None:\n",
    "        if coltype == 'numeric':\n",
    "            summ = _numeric_summary(column)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            summ = _categorical_summary(column, 5)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if coltype == 'numeric':\n",
    "            column_list = []\n",
    "\n",
    "            vals = by.dropna().unique()\n",
    "            for value in vals:\n",
    "                subcol = column[by == value]\n",
    "                summcol = _numeric_summary(subcol)\n",
    "                column_list.append(summcol)\n",
    "\n",
    "            summ = pd.DataFrame(column_list, index=vals)\n",
    "\n",
    "        elif coltype == 'object':\n",
    "            subcol = column.groupby(by)\n",
    "            summ = _categorical_summary(subcol)\n",
    "            #summ = _summ.sort_values(by=subcol)\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def reproduction_number(G, index_cases=True, plot=True):\n",
    "    \"\"\"\n",
    "    Finds each case's basic reproduction number, which is the number of secondary\n",
    "    infections each case produces.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    G = networkx object\n",
    "    index_cases = include index nodes, i.e. those at generation 0. Default is True.\n",
    "                  Excluding them is useful if you want to calculate the human to human\n",
    "                  reproduction number without considering zoonotically acquired cases.\n",
    "    summary = print summary statistics of the case reproduction numbers\n",
    "    plot = create histogram of case reproduction number distribution.\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    pandas series of case reproduction numbers and matplotlib figure\n",
    "    and axis objects if plot=True\n",
    "    \"\"\"\n",
    "\n",
    "    if index_cases == True:\n",
    "        R = pd.Series(G.out_degree())\n",
    "\n",
    "    elif index_cases == False:\n",
    "        degrees = {}\n",
    "\n",
    "        for n in G.node:\n",
    "            if G.node[n]['generation'] > 0:\n",
    "                degrees[n] = G.out_degree(n)\n",
    "        R = pd.Series(degrees)\n",
    "\n",
    "    print('Summary of reproduction numbers')\n",
    "    print(R.describe(), '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        R.hist(ax=ax, alpha=.5)\n",
    "        ax.set_xlabel('Secondary cases')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.grid(False)\n",
    "        return R, fig, ax\n",
    "\n",
    "    else:\n",
    "        return R\n",
    "\n",
    "\n",
    "def generation_analysis(G, attribute, plot=True):\n",
    "    \"\"\"\n",
    "    Analyzes an attribute, e.g. health status, by generation.\n",
    "    PARAMETERS\n",
    "    -------------\n",
    "    G = networkx object\n",
    "    attribute = case attribute for analysis, e.g. health status or sex\n",
    "    table = print cross table of attribute by generation. Default is true.\n",
    "    plot = produce histogram of attribute by generation. Default is true.\n",
    "    RETURNS\n",
    "    --------------\n",
    "    matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "\n",
    "    gen_df = pd.DataFrame(G.node).T\n",
    "\n",
    "    print('{} by generation').format(attribute)\n",
    "    table = pd.crosstab(gen_df.generation, gen_df[attribute], margins=True)\n",
    "    print(table, '\\n')\n",
    "\n",
    "    if plot == True:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_aspect('auto')\n",
    "        pd.crosstab(gen_df.generation, gen_df[attribute]).plot(kind='bar', ax=ax, alpha=.5, rot=0)\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel('Case count')\n",
    "        ax.grid(False)\n",
    "        ax.legend(loc='best');\n",
    "        return fig, ax, table\n",
    "    else:\n",
    "        return table\n",
    "\n",
    "\n",
    "def create_2x2(df, row, column, row_order, col_order):\n",
    "    \"\"\"\n",
    "    2x2 table of disease and exposure in traditional epi order.\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    PARAMETERS\n",
    "    -----------------------\n",
    "    df = pandas dataframe of line listing\n",
    "    row = name of exposure row as string\n",
    "    column = name of outcome column as string\n",
    "    row_order = list of length 2 of row values in yes/no order.\n",
    "                Example: ['Exposed', 'Unexposed']\n",
    "    col_order = list of length 2 column values in yes/no order.\n",
    "                Example: ['Sick', 'Not sick']\n",
    "    RETURNS\n",
    "    ------------------------\n",
    "    pandas dataframe of 2x2 table. Prints odds ratio and relative risk.\n",
    "    \"\"\"\n",
    "    if type(col_order) != list or type(row_order) != list:\n",
    "        raise TypeError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    if len(col_order) != 2 or len(row_order) != 2:\n",
    "        raise AssertionError('row_order and col_order must each be lists of length 2')\n",
    "\n",
    "    _table = pd.crosstab(df[row], df[column], margins=True).to_dict()\n",
    "\n",
    "    trow = row_order[0]\n",
    "    brow = row_order[1]\n",
    "    tcol = col_order[0]\n",
    "    bcol = col_order[1]\n",
    "\n",
    "    table = pd.DataFrame(_table, index=[trow, brow, 'All'], columns=[tcol, bcol, 'All'])\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def analyze_2x2(table):\n",
    "    \"\"\"\n",
    "    Prints odds ratio, relative risk, and chi square.\n",
    "    See also create_2x2(), odds_ratio(), relative_risk(), and chi2()\n",
    "    PARAMETERS\n",
    "    --------------------\n",
    "    2x2 table as pandas dataframe, numpy array, or list in format [a, b, c, d]\n",
    "    Table format:\n",
    "                Disease\n",
    "    Exposure    YES     NO\n",
    "    YES         a       b\n",
    "    NO          c       d\n",
    "    \"\"\"\n",
    "\n",
    "    odds_ratio(table)\n",
    "    relative_risk(table)\n",
    "    attributable_risk(table)\n",
    "    chi2(table)\n",
    "\n",
    "\n",
    "def odds_ratio(table):\n",
    "    \"\"\"\n",
    "    Calculates the odds ratio and 95% confidence interval. See also\n",
    "    analyze_2x2()\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints odds ratio and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    ratio = (a*d)/(b*c)\n",
    "    or_se = np.sqrt((1/a)+(1/b)+(1/c)+(1/d))\n",
    "    or_ci = _conf_interval(ratio, or_se)\n",
    "    print('Odds ratio: {} (95% CI: {})'.format(round(ratio, 2), or_ci))\n",
    "\n",
    "    return round(ratio, 2), or_ci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def relative_risk(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the relative risk and 95% confidence interval. See also\n",
    "    analyze_2x2().\n",
    "    *Cells in the table with a value of 0 will be replaced with .1\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints relative risk and tuple of 95% confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    rr = (a/(a+b))/(c/(c+d))\n",
    "    rr_se = np.sqrt(((1/a)+(1/c)) - ((1/(a+b)) + (1/(c+d))))\n",
    "    rr_ci = _conf_interval(rr, rr_se)\n",
    "\n",
    "    if display is not False:\n",
    "        print('Relative risk: {} (95% CI: {}-{})\\n'.format(round(rr, 2), round(rr_ci[0],2), round(rr_ci[1], 2)))\n",
    "\n",
    "    return rr, rr_ci\n",
    "\n",
    "\n",
    "def attributable_risk(table):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "    print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "    print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "    print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return ar, arp, par, parp\n",
    "\n",
    "\n",
    "def my_attributable_risk(table, print_result =  True):\n",
    "    \"\"\"\n",
    "    Calculate the attributable risk, attributable risk percent,\n",
    "    and population attributable risk.\n",
    "    PARAMETERS\n",
    "    ----------------\n",
    "    table = 2x2 table. See 2x2_table()\n",
    "    RETURNS\n",
    "    ----------------\n",
    "    prints and returns attributable risk (AR), attributable risk percent\n",
    "    (ARP), population attributable risk (PAR) and population attributable\n",
    "    risk percent (PARP).\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    N = a + b + c + d\n",
    "\n",
    "    ar = (a/(a+b))-(c/(c+d))\n",
    "    ar_se = np.sqrt(((a+c)/N)*(1-((a+c)/N))*((1/(a+b))+(1/(c+d))))\n",
    "    ar_ci = (round(ar-(1.96*ar_se), 2), round(ar+(1.96*ar_se), 2))\n",
    "\n",
    "    rr, rci = relative_risk(table, display=False)\n",
    "    arp = 100*((rr-1)/(rr))\n",
    "    arp_se = (1.96*ar_se)/ar\n",
    "    arp_ci = (round(arp-arp_se, 2), round(arp+arp_se, 3))\n",
    "\n",
    "    par = ((a+c)/N) - (c/(c+d))\n",
    "    parp = 100*(par/(((a+c)/N)))\n",
    "\n",
    "    if(print_result):\n",
    "        print('Attributable risk: {} (95% CI: {})'.format(round(ar, 3), ar_ci))\n",
    "        print('Attributable risk percent: {}% (95% CI: {})'.format(round(arp, 2), arp_ci))\n",
    "        print('Population attributable risk: {}'.format(round(par, 3)))\n",
    "        print('Population attributable risk percent: {}% \\n'.format(round(parp, 2)))\n",
    "\n",
    "    return arp, arp_ci\n",
    "\n",
    "def chi2(table):\n",
    "    \"\"\"\n",
    "    Scipy.stats function to calculate chi square.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe or numpy array. See also\n",
    "    analyze_2x2().\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns chi square with yates correction, p value,\n",
    "    degrees of freedom, and array of expected values.\n",
    "    prints chi square and p value\n",
    "    \"\"\"\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "    #print('Chi square: {}'.format(chi2))\n",
    "    #print('p value: {}'.format(p))\n",
    "\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "\n",
    "def summary(data, by=None):\n",
    "    \"\"\"\n",
    "    Displays approporiate summary statistics for each column in a line listing.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    data = pandas data frame or series\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    for each column in the dataframe, or for hte series:\n",
    "    - if column data type is numeric, returns summary statistics\n",
    "    - if column data type is non-numeric, returns count and frequency of\n",
    "        top 5 most common values.\n",
    "    EXAMPLE\n",
    "    ----------------------\n",
    "    df = pd.DataFrame({'Age' : [10, 12, 14], 'Group' : ['A', 'B', 'B'] })\n",
    "    In: summary(df.Age)\n",
    "    Out:\n",
    "        count       3\n",
    "        missing     0\n",
    "        min        10\n",
    "        median     12\n",
    "        mean       12\n",
    "        std         2\n",
    "        max        14\n",
    "        dtype: float64\n",
    "    In: summary(df.Group)\n",
    "    Out:\n",
    "           count      freq\n",
    "        B      2  0.666667\n",
    "        A      1  0.333333\n",
    "    In:summary(df.Age, by=df.Group)\n",
    "    Out     count  missing  min  median  mean      std  max\n",
    "        A      1        0   10      10    10       NaN   10\n",
    "        B      2        0   12      13    13  1.414214   14\n",
    "    \"\"\"\n",
    "    if type(data) == pd.core.series.Series:\n",
    "        summ = _summary_calc(data, by=by)\n",
    "        return summ\n",
    "\n",
    "    elif type(data) == pd.core.frame.DataFrame:\n",
    "        for column in data:\n",
    "            summ = _summary_calc(data[column], by=None)\n",
    "            print('----------------------------------')\n",
    "            print(column, '\\n')\n",
    "            print(summ)\n",
    "\n",
    "\n",
    "def diagnostic_accuracy(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculates the sensitivity, specificity, negative and positive predictive values\n",
    "    of a 2x2 table with 95% confidence intervals. Note that confidence intervals\n",
    "    are made based on a normal approximation, and may not be appropriate for\n",
    "    small sample sizes.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints diagnostic accuracy estimates and tuple of 95% confidence interval\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "\n",
    "    sen = (a/(a+c))\n",
    "    sen_se = np.sqrt((sen*(1-sen))/(a+c))\n",
    "    sen_ci = (sen-(1.96*sen_se),sen+(1.96*sen_se))\n",
    "    spec = (d/(b+d))\n",
    "    spec_se = np.sqrt((spec*(1-spec))/(b+d))\n",
    "    spec_ci = (spec-(1.96*spec_se),spec+(1.96*spec_se))\n",
    "    PPV = (a/(a+b))\n",
    "    PPV_se = np.sqrt((PPV*(1-PPV))/(a+b))\n",
    "    PPV_ci = (PPV-(1.96*PPV_se),PPV+(1.96*PPV_se))\n",
    "    NPV = (d/(c+d))\n",
    "    NPV_se = np.sqrt((NPV*(1-NPV))/(c+d))\n",
    "    NPV_ci = (NPV-(1.96*NPV_se),NPV+(1.96*NPV_se))\n",
    "\n",
    "    if display is not False:\n",
    "        print('Sensitivity: {} (95% CI: {})\\n'.format(round(sen, 2), sen_ci))\n",
    "        print('Specificity: {} (95% CI: {})\\n'.format(round(spec, 2), spec_ci))\n",
    "        print('Positive Predictive Value: {} (95% CI: {})\\n'.format(round(PPV, 2), PPV_ci))\n",
    "        print('Negative Predictive Value: {} (95% CI: {})\\n'.format(round(NPV, 2), NPV_ci))\n",
    "\n",
    "    return sen,sen_ci,spec,spec_ci,PPV,PPV_ci,NPV,NPV_ci\n",
    "\n",
    "\n",
    "def kappa_agreement(table, display=True):\n",
    "    \"\"\"\n",
    "    Calculated an unweighted Cohen's kappa statistic of observer agreement for a 2x2 table.\n",
    "    Note that the kappa statistic can be extended to an n x m table, but this\n",
    "    implementation is restricted to 2x2.\n",
    "    PARAMETERS\n",
    "    ----------------------\n",
    "    table = accepts pandas dataframe, numpy array, or list in [a, b, c, d] format.\n",
    "    RETURNS\n",
    "    ----------------------\n",
    "    returns and prints the Kappa statistic\n",
    "    Author: Eric Lofgren\n",
    "    \"\"\"\n",
    "    a, b, c, d = _ordered_table(table)\n",
    "    n = a + b + c + d\n",
    "    pr_a = ((a+d)/n)\n",
    "    pr_e = (((a+b)/n) * ((a+c)/n)) + (((c+d)/n) * ((b+d)/n))\n",
    "    k = (pr_a - pr_e)/(1 - pr_e)\n",
    "    if display is not False:\n",
    "        print(\"Cohen's Kappa: {}\\n\").format(round(k, 2))\n",
    "\n",
    "    return k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ab64c-d041-4e56-9164-76363c16e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(coef_df_all, results_labels_df_all, title = ''):\n",
    "\n",
    "    df = coef_df_all.iloc[1::, 0:2].sort_values(by='Estimate mean', ascending=False)\n",
    "    \n",
    "    colors_palette = get_bar_colors(df.reset_index().rename(columns={'index':'Features'}), X_cat_)\n",
    "    sns.barplot(x='Estimate mean', y=df.index, data=df, palette=colors_palette, orient='h')\n",
    "    plt.xlabel('Estimate mean')\n",
    "    plt.ylabel('Variable')\n",
    "    #plt.title('Estimate mean con Error Bars (Estimate std)')\n",
    "    plt.xlim([0, 0.55])\n",
    "    \n",
    "    \n",
    "    r2 = np.round(coef_df_all.loc['_intercept', 'R2'],2)\n",
    "    f2 = np.round(coef_df_all.loc['_intercept', 'F2'],2)\n",
    "    rmse = np.round(coef_df_all.loc['_intercept', 'rmse'],2)\n",
    "    mde = np.round(coef_df_all.loc['_intercept', 'MDE'],2)\n",
    "    mae = np.round(coef_df_all.loc['_intercept', 'MAE'],2)\n",
    "    \n",
    "    y_true = results_labels_df_all['y_labels']\n",
    "    y_pred = results_labels_df_all['y_pred']\n",
    "    \n",
    "    correlacion, p_value = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    plt.text( 0.23, df.shape[0]-6, r'$R^{2}$: ' + str(np.round(r2, 2)), va='center', ha='left')\n",
    "    plt.text( 0.23, df.shape[0]-5, r'$F^{2}$: ' + str(np.round(f2, 2)), va='center', ha='left')\n",
    "    plt.text( 0.23, df.shape[0]-4, f'corr: {correlacion:.2f}', va='center', ha='left')\n",
    "    plt.text( 0.23, df.shape[0]-3, f'RMSE: {rmse:.2f}', va='center', ha='left')\n",
    "    plt.text( 0.23, df.shape[0]-2, f'MDE: {mde:.2f}', va='center', ha='left')\n",
    "    plt.text( 0.23, df.shape[0]-1, f'MAE: {mae:.2f}', va='center', ha='left')\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def plot_coef_v01(coef_df_for_plot, results_labels_df_for_plot, df_directions_for_plot):\n",
    "\n",
    "    df_directions_for_plot = df_directions_for_plot.set_index('index')\n",
    "    list_coef = list(coef_df_for_plot.index)\n",
    "    list_coef.remove('_intercept')\n",
    "    coef_df_for_orig = coef_df_for_plot.copy()\n",
    "    coef_df_for_plot['coef_sign'] = 0\n",
    "    \n",
    "    for i in list_coef:\n",
    "        sign =  df_directions_for_plot.loc[i, 'coef']\n",
    "        if(sign<0):\n",
    "            sign = -1\n",
    "        else:\n",
    "            sign = 1\n",
    "    \n",
    "        coef_df_for_plot.loc[i, 'coef_sign'] = coef_df_for_plot.loc[i, 'Estimate mean']*sign\n",
    "    \n",
    "    coef_df_for_plot['color'] = coef_df_for_plot['coef_sign'].apply(lambda x: 'red' if x > 0 else 'blue')\n",
    "    \n",
    "    coef_df_for_plot_sort = coef_df_for_plot.iloc[1::, :].sort_values(by='Estimate mean', ascending=False)\n",
    "    coef_df_for_plot_sort = coef_df_for_plot_sort[['Estimate mean', 'Estimate std', 'coef_sign', 'color']]\n",
    "    coef_df_for_plot_sort.reset_index(inplace = True)\n",
    "    \n",
    "    #plt.figure(figsize=(3, 4))\n",
    "\n",
    "    if(coef_df_for_plot_sort.loc[0, 'color'] == 'blue'):\n",
    "        sns.barplot(x='coef_sign', y='index', data=coef_df_for_plot_sort, hue='color', dodge=False, palette=['royalblue', 'firebrick'], orient='h' ,legend=False)\n",
    "    else:\n",
    "        sns.barplot(x='coef_sign', y='index', data=coef_df_for_plot_sort, hue='color', dodge=False, palette=['firebrick', 'royalblue'], orient='h' ,legend=False)\n",
    "        \n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.xlim([-0.3, 0.3])\n",
    "    plt.xlabel('Features importance')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    r2 = np.round(coef_df_for_orig.loc['_intercept', 'R2'],4)\n",
    "    f2 = np.round(coef_df_for_orig.loc['_intercept', 'F2'],4)\n",
    "    rmse = np.round(coef_df_for_orig.loc['_intercept', 'rmse'],4)\n",
    "    try:\n",
    "        mde = np.round(coef_df_for_orig.loc['_intercept', 'MDE'],4)\n",
    "    except:\n",
    "        mde = 0\n",
    "    mae = np.round(coef_df_for_orig.loc['_intercept', 'MAE'],4)\n",
    "    \n",
    "    y_true = results_labels_df_for_plot['y_labels']\n",
    "    y_pred = results_labels_df_for_plot['y_pred']\n",
    "    \n",
    "    correlacion, p_value = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    plt.title(r'$R^{2}$: ' + str(np.round(r2, 4)) + r'   $F^{2}$: ' + str(np.round(f2, 4)) + f'   $r$: {correlacion:.4f}' + f'\\nRMSE: {rmse:.4f}' + f'   MDE: {mde:.4f}'+ f'   MAE: {mae:.4f}');\n",
    "    \n",
    "    plt.text( -0.20, 10, 'Protected\\nFactors', va='center', ha='center', color='blue')\n",
    "    plt.text( 0.20, 10, 'Risk\\nFactors', va='center', ha='center', color='red')\n",
    "\n",
    "    return coef_df_for_plot_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_directional_accuracy(y_true, y_pred):\n",
    "    \n",
    "    differences = np.array(y_pred) - np.array(y_true) \n",
    "    signs = np.sign(differences)\n",
    "    mde = np.mean(signs)\n",
    "    return mde\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    absolute_errors = np.abs(y_pred - y_true)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def coef_pval(coef_array_mean_, X_, y_, y_p):\n",
    "\n",
    "    n = X_.shape[0]\n",
    "    t = coef_tval(coef_array_mean_, X_, y_, y_p)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "\n",
    "def coef_tval(coef_array_mean_, X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_tval for OLS of statsmodels\n",
    "    '''\n",
    "    \n",
    "    a = np.array(coef_array_mean_[0][0]/ coef_se(X_, y_, y_p)[0])\n",
    "    b = np.array(coef_array_mean_[1::].flatten() / coef_se(X_, y_, y_p)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "\n",
    "def coef_se(X_, y_, y_p):\n",
    "    \n",
    "    '''\n",
    "        coef_se for OLS of statsmodels\n",
    "    '''\n",
    "    n = X_.shape[0]\n",
    "    \n",
    "    X1 = np.hstack((np.ones((n, 1)), np.matrix(X_)))\n",
    "    se_matrix = scipy.linalg.sqrtm(\n",
    "        metrics.mean_squared_error(y_, y_p) *\n",
    "        np.linalg.inv(X1.T * X1)\n",
    "    )\n",
    "    return np.diagonal(se_matrix)\n",
    "\n",
    "def directional_accuracy(predicted_values, true_values):\n",
    "\n",
    "    predicted_values = np.array(predicted_values)\n",
    "    true_values = np.array(true_values)    \n",
    "\n",
    "    difference_direction = np.sign(predicted_values - true_values)    \n",
    "    correct_direction_count = np.sum(difference_direction == 1) + np.sum(difference_direction == -1)\n",
    "    \n",
    "    directional_accuracy_score = correct_direction_count / len(predicted_values)\n",
    "    \n",
    "    return directional_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsmodels.api import GLM\n",
    "from statsmodels.api import families\n",
    "\n",
    "\n",
    "def Regression_GBR(X, y, min_, max_, n_splits, params_b = -1, shaps_comp = False):\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    #scaling_data = scaler.fit_transform(X)\n",
    "    #X = pd.DataFrame(scaling_data, columns= X.columns, index = X.index)\n",
    "\n",
    "    for i in range(min_, max_):\n",
    "        y_labels = []\n",
    "        y_predicts = []\n",
    "        #n_splits = 7\n",
    "\n",
    "        y_pred_ = []\n",
    "        y_test_ = []\n",
    "        r_squared_l = []\n",
    "        rmse_l = []\n",
    "        mse_l = []\n",
    "\n",
    "        results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "\n",
    "        r_squared_ = 0\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=i)\n",
    "\n",
    "        lista_vars = list(X)\n",
    "\n",
    "        coef_array = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_t_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "        coef_p_value = np.zeros([len(lista_vars)+1, n_splits])\n",
    "\n",
    "        iter_ = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "                import warnings\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "                print(f\"Fold: {iter_}\")\n",
    "            \n",
    "                X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                scaling_data = scaler.fit_transform(X_train)\n",
    "                X_train = pd.DataFrame(scaling_data, columns= X_train.columns, index = X_train.index)\n",
    "\n",
    "                scaling_data = scaler.transform(X_test)\n",
    "                X_test = pd.DataFrame(scaling_data, columns= X_test.columns, index = X_test.index)\n",
    "\n",
    "                if(params_b == -1):\n",
    "                    model = GradientBoostingRegressor(random_state=42)\n",
    "                else:\n",
    "                    model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "                    \n",
    "\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "                coef_array[0,iter_] = np.nan\n",
    "                coef_array[1::,iter_] = np.array(model.feature_importances_)\n",
    "\n",
    "                predicted_values = model.predict(X_test)\n",
    "\n",
    "\n",
    "                mse = np.mean((y_test - predicted_values)**2)\n",
    "\n",
    "                rmse = np.sqrt(mse)\n",
    "\n",
    "                y_labels.extend(list(y_test))\n",
    "                y_predicts.extend(list(predicted_values))\n",
    "\n",
    "                #r_squared_ += results.rsquared\n",
    "\n",
    "\n",
    "                y_pred_.extend(list(predicted_values))\n",
    "                y_test_.extend(y_test)\n",
    "                \n",
    "                 #- No Adjust gap------------------------\n",
    "                gap_test =  predicted_values - y_test\n",
    "                 #- No Adjust gap------------------------\n",
    "                    \n",
    "                \n",
    "                #- Adjust gap------------------------    \n",
    "\n",
    "                #gap_train = model.predict(X_train) - y_train\n",
    "                #model_gap = GLM(gap_train, y_train, family=families.Gaussian())\n",
    "                #results_gap = model_gap.fit()\n",
    "                #corrected_gap = gap_test - results_gap.predict(gap_test)\n",
    "\n",
    "                gap_train =  model.predict(X_train) - y_train\n",
    "\n",
    "\n",
    "                slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "\n",
    "                corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "                 #- Adjust gap------------------------\n",
    "                  \n",
    "                r_squared_l.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "                mse_l.append(np.round(mean_squared_error(y_test, model.predict(X_test)), 6))\n",
    "                rmse_l.append(np.round(math.sqrt(mean_squared_error(y_test, model.predict(X_test))), 6))\n",
    "\n",
    "                result = np.column_stack((y_test, model.predict(X_test), gap_test, corrected_gap))\n",
    "                temp_df = pd.DataFrame(result, columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected'])\n",
    "                temp_df['ID'] = X_test.index\n",
    "\n",
    "                results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "                iter_+=1\n",
    "\n",
    "        n = len(y_predicts)\n",
    "        p = X.shape[1]\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "        \n",
    "        mde = mean_directional_accuracy(y_labels, y_predicts)\n",
    "        mae = mean_absolute_error(y_labels, y_predicts)\n",
    "        \n",
    "        k = X.shape[1] - 1\n",
    "        r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "\n",
    "        mse  = (np.round(mean_squared_error(y_labels, y_predicts), 6))\n",
    "        rmse = (np.round(math.sqrt(mean_squared_error(y_labels, y_predicts)), 6))\n",
    "\n",
    "        r_squared_ = r_squared_/n_splits\n",
    "        F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "        p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "\n",
    "        F2 =r_squared  / (1 - r_squared)\n",
    "\n",
    "        coef_array_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "        coef_array_std = np.zeros([len(lista_vars)+1, 1])\n",
    "\n",
    "        for j in range(len(lista_vars)+1):\n",
    "            coef_array_mean[j] = coef_array[j,:].mean()\n",
    "            coef_array_std[j] = coef_array[j,:].std()\n",
    "\n",
    "        coef_df = pd.DataFrame(\n",
    "                                index= ['_intercept'] + lista_vars,\n",
    "                                columns=['Estimate mean', 'Estimate std','t value', 'p value'])\n",
    "\n",
    "        coef_df['Estimate mean'] = coef_array_mean\n",
    "        coef_df['Estimate std'] = coef_array_std\n",
    "        coef_df['t value'] = coef_tval(coef_array_mean, X, y_labels, y_predicts)\n",
    "        coef_df['p value'] = coef_pval(coef_array_mean, X, y_labels, y_predicts)\n",
    "\n",
    "        coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "        coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "        coef_df.loc['_intercept', 'R2 [+-]'] = 1*np.std(r_squared_l)\n",
    "        coef_df.loc['_intercept', 'F2'] = F2\n",
    "        coef_df.loc['_intercept', 'mse'] = mse\n",
    "        coef_df.loc['_intercept', 'mse [+-]']  = 1*np.std(mse_l)\n",
    "        coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "        coef_df.loc['_intercept', 'rmse [+-]'] = 1*np.std(rmse_l)\n",
    "        coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "        #\n",
    "        coef_df.loc['_intercept', 'F'] = F\n",
    "        coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "        \n",
    "        coef_df.loc['_intercept', 'MDE'] = mde\n",
    "        coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "        r_squared = r2_score(y_labels, y_predicts)\n",
    "        #if(r_squared>0.1):\n",
    "        #    print(i, np.round(r_squared_, 3), np.round(r_squared,3))\n",
    "        \n",
    "        \n",
    "    results_labels_df['y_pred_corrected'] =  results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID','y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "    \n",
    "    if(shaps_comp):\n",
    "        if(params_b == -1):\n",
    "            model = GradientBoostingRegressor(random_state=42)\n",
    "        else:\n",
    "            model = GradientBoostingRegressor(random_state=42, **params_b)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        explainer = shap.Explainer(model, X)\n",
    "        #shap_values = explainer(X)\n",
    "            \n",
    "        return [coef_df, r_squared_, results_labels_df, explainer, kf.split(X)]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df, kf.split(X)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70592654-7f68-4198-9617-3ea0ca017883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import shap\n",
    "import warnings\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "def Regression_GBR_nested(X, y, outer_splits=10, inner_splits=5, shaps_comp=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "\n",
    "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': Integer(50, 500),          # Número de árboles\n",
    "        'learning_rate': Real(0.01, 0.5, prior='log-uniform'),  # Tasa de aprendizaje\n",
    "        'max_depth': Integer(1, 5),               # Profundidad máxima de los árboles\n",
    "        'min_samples_split': Integer(2, 10),       # Mínimo de muestras para dividir un nodo\n",
    "        'min_samples_leaf': Integer(1, 10),        # Mínimo de muestras en una hoja\n",
    "    }\n",
    "\n",
    "    y_labels = []\n",
    "    y_predicts = []\n",
    "    r_squared_l = []\n",
    "    mse_l = []\n",
    "    rmse_l = []\n",
    "\n",
    "    results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "\n",
    "    coef_array = np.zeros([X.shape[1] + 1, outer_splits])\n",
    "    lista_vars = list(X.columns)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        print(f\"Fold: {fold}\")\n",
    "        \n",
    "        scaling_data = scaler.fit_transform(X_train)\n",
    "        X_train = pd.DataFrame(scaling_data, columns= X_train.columns, index = X_train.index)\n",
    "\n",
    "        scaling_data = scaler.transform(X_test)\n",
    "        X_test = pd.DataFrame(scaling_data, columns= X_test.columns, index = X_test.index)\n",
    "        \n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "        inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=model,\n",
    "            search_spaces=param_grid,\n",
    "            n_iter=3,              \n",
    "            scoring='r2',  \n",
    "            cv=inner_cv,                   \n",
    "            n_jobs=-1,             \n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = bayes_search.best_estimator_\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        gap_test = y_pred - y_test\n",
    "        gap_train = best_model.predict(X_train) - y_train\n",
    "\n",
    "        slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "        corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "        result = np.column_stack((y_test, y_pred, gap_test, corrected_gap))\n",
    "        temp_df = pd.DataFrame(result, columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected'])\n",
    "        temp_df['ID'] = X_test.index\n",
    "        results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "        y_labels.extend(y_test)\n",
    "        y_predicts.extend(y_pred)\n",
    "\n",
    "        r_squared_l.append(r2_score(y_test, y_pred))\n",
    "        mse_l.append(mean_squared_error(y_test, y_pred))\n",
    "        rmse_l.append(math.sqrt(mse_l[-1]))\n",
    "\n",
    "        coef_array[0, fold] = np.nan\n",
    "        coef_array[1:, fold] = best_model.feature_importances_\n",
    "\n",
    "    # Métricas finales\n",
    "    y_labels = np.array(y_labels)\n",
    "    y_predicts = np.array(y_predicts)\n",
    "\n",
    "    r_squared = r2_score(y_labels, y_predicts)\n",
    "    r_squared_ = np.mean(r_squared_l)\n",
    "    n = len(y_labels)\n",
    "    p = X.shape[1]\n",
    "    k = p - 1\n",
    "\n",
    "    r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "    mse = np.mean(mse_l)\n",
    "    rmse = np.mean(rmse_l)\n",
    "    mae = mean_absolute_error(y_labels, y_predicts)\n",
    "    F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "    p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "    F2 = r_squared / (1 - r_squared)\n",
    "\n",
    "    # Coeficientes medios\n",
    "    coef_df = pd.DataFrame(index=['_intercept'] + lista_vars,\n",
    "                           columns=['Estimate mean', 'Estimate std', 't value', 'p value'])\n",
    "\n",
    "    coef_df['Estimate mean'] = coef_array.mean(axis=1)\n",
    "    coef_df['Estimate std'] = coef_array.std(axis=1)\n",
    "    #coef_df['t value'] = coef_tval(coef_array.mean(axis=1), X, y_labels, y_predicts)\n",
    "    #coef_df['p value'] = coef_pval(coef_array.mean(axis=1), X, y_labels, y_predicts)\n",
    "\n",
    "    coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "    coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "    coef_df.loc['_intercept', 'R2 [+-]'] = np.std(r_squared_l)\n",
    "    coef_df.loc['_intercept', 'F2'] = F2\n",
    "    coef_df.loc['_intercept', 'mse'] = mse\n",
    "    coef_df.loc['_intercept', 'mse [+-]'] = np.std(mse_l)\n",
    "    coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "    coef_df.loc['_intercept', 'rmse [+-]'] = np.std(rmse_l)\n",
    "    coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "    coef_df.loc['_intercept', 'F'] = F\n",
    "    coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "    coef_df.loc['_intercept', 'MDE'] = mean_directional_accuracy(y_labels, y_predicts)\n",
    "    coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "    results_labels_df['y_pred_corrected'] = results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID','y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "\n",
    "    if shaps_comp:\n",
    "        explainer = shap.Explainer(best_model, X)\n",
    "        return [coef_df, r_squared_, results_labels_df, explainer, outer_cv.split(X)]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df, outer_cv.split(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72e190-0990-40a9-9645-a94cc27afee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import linregress\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "def Regression_GBR_leave_one_site_out(X, y, groups, inner_splits=5, shaps_comp=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "\n",
    "    outer_cv = LeaveOneGroupOut()\n",
    "    param_grid = {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "        'max_depth': Integer(1, 5),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "    }\n",
    "\n",
    "    y_labels = []\n",
    "    y_predicts = []\n",
    "    r_squared_l = []\n",
    "    mse_l = []\n",
    "    rmse_l = []\n",
    "\n",
    "    results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "\n",
    "    coef_array = np.zeros([X.shape[1] + 1, len(np.unique(groups))])\n",
    "    lista_vars = list(X.columns)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y, groups=groups)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        vc = groups.iloc[test_idx].value_counts()\n",
    "        country_name = vc.index[0]\n",
    "        n_obs = vc.values[0]\n",
    "        print(f\"Fold: {fold} {country_name} - {n_obs}\")\n",
    "\n",
    "\n",
    "\n",
    "        scaling_data = scaler.fit_transform(X_train)\n",
    "        X_train = pd.DataFrame(scaling_data, columns= X_train.columns, index = X_train.index)\n",
    "\n",
    "        scaling_data = scaler.transform(X_test)\n",
    "        X_test = pd.DataFrame(scaling_data, columns= X_test.columns, index = X_test.index)\n",
    "\n",
    "\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "        inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=model,\n",
    "            search_spaces=param_grid,\n",
    "            n_iter=3,\n",
    "            scoring='r2',\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        best_model = bayes_search.best_estimator_\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        gap_test = y_pred - y_test\n",
    "        gap_train = best_model.predict(X_train) - y_train\n",
    "\n",
    "        slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "        corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "        result = np.column_stack((y_test, y_pred, gap_test, corrected_gap))\n",
    "        temp_df = pd.DataFrame(result, columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected'])\n",
    "        temp_df['ID'] = X_test.index\n",
    "        results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "        y_labels.extend(y_test)\n",
    "        y_predicts.extend(y_pred)\n",
    "\n",
    "        r_squared_l.append(r2_score(y_test, y_pred))\n",
    "        mse_l.append(mean_squared_error(y_test, y_pred))\n",
    "        rmse_l.append(math.sqrt(mse_l[-1]))\n",
    "\n",
    "        coef_array[0, fold] = np.nan\n",
    "        coef_array[1:, fold] = best_model.feature_importances_\n",
    "\n",
    "    # Métricas finales\n",
    "    y_labels = np.array(y_labels)\n",
    "    y_predicts = np.array(y_predicts)\n",
    "\n",
    "    r_squared = r2_score(y_labels, y_predicts)\n",
    "    r_squared_ = np.mean(r_squared_l)\n",
    "    n = len(y_labels)\n",
    "    p = X.shape[1]\n",
    "    k = p - 1\n",
    "\n",
    "    r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "    mse = np.mean(mse_l)\n",
    "    rmse = np.mean(rmse_l)\n",
    "    mae = mean_absolute_error(y_labels, y_predicts)\n",
    "    F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "    p_value = np.round(math.exp(-F), 15)  # aproximación si scipy.stats no está disponible\n",
    "    F2 = r_squared / (1 - r_squared)\n",
    "\n",
    "    coef_df = pd.DataFrame(index=['_intercept'] + lista_vars,\n",
    "                           columns=['Estimate mean', 'Estimate std', 't value', 'p value'])\n",
    "\n",
    "    coef_df['Estimate mean'] = coef_array.mean(axis=1)\n",
    "    coef_df['Estimate std'] = coef_array.std(axis=1)\n",
    "    # coef_df['t value'] = coef_tval(...)\n",
    "    # coef_df['p value'] = coef_pval(...)\n",
    "\n",
    "    coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "    coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "    coef_df.loc['_intercept', 'R2 [+-]'] = np.std(r_squared_l)\n",
    "    coef_df.loc['_intercept', 'F2'] = F2\n",
    "    coef_df.loc['_intercept', 'mse'] = mse\n",
    "    coef_df.loc['_intercept', 'mse [+-]'] = np.std(mse_l)\n",
    "    coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "    coef_df.loc['_intercept', 'rmse [+-]'] = np.std(rmse_l)\n",
    "    coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "    coef_df.loc['_intercept', 'F'] = F\n",
    "    coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "    coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "    results_labels_df['y_pred_corrected'] = results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID','y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "\n",
    "    if shaps_comp:\n",
    "        explainer = shap.Explainer(best_model, X)\n",
    "        return [coef_df, r_squared_, results_labels_df, explainer, outer_cv.split(X)]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df, outer_cv.split(X, y, groups=groups)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ceac0-5baa-45a0-a30c-9789a0be7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import linregress\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import shap\n",
    "import warnings\n",
    "import scipy\n",
    "\n",
    "def mean_directional_accuracy_heldout(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1]))\n",
    "\n",
    "def Regression_GBR_nested_heldout(X, y, X_heldout=None, y_heldout=None, outer_splits=10, inner_splits=5, shaps_comp=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "        'max_depth': Integer(1, 5),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "    }\n",
    "\n",
    "    y_labels, y_predicts = [], []\n",
    "    r_squared_l, mse_l, rmse_l = [], [], []\n",
    "\n",
    "    results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "    heldout_all_df = pd.DataFrame(columns=['ID', 'y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected'])\n",
    "    coef_array = np.zeros([X.shape[1] + 1, outer_splits])\n",
    "    lista_vars = list(X.columns)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        print(f\"Fold: {fold}\")\n",
    "        \n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "        inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=model,\n",
    "            search_spaces=param_grid,\n",
    "            n_iter=3,\n",
    "            scoring='r2',\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        best_model = bayes_search.best_estimator_\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        gap_test = y_pred - y_test\n",
    "        gap_train = best_model.predict(X_train) - y_train\n",
    "\n",
    "        slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "        corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            'y_labels': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'GAP': gap_test,\n",
    "            'GAP_corrected': corrected_gap,\n",
    "            'ID': X_test.index\n",
    "        })\n",
    "        results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "        y_labels.extend(y_test)\n",
    "        y_predicts.extend(y_pred)\n",
    "\n",
    "        r_squared_l.append(r2_score(y_test, y_pred))\n",
    "        mse_l.append(mean_squared_error(y_test, y_pred))\n",
    "        rmse_l.append(math.sqrt(mse_l[-1]))\n",
    "\n",
    "        coef_array[0, fold] = np.nan\n",
    "        coef_array[1:, fold] = best_model.feature_importances_\n",
    "\n",
    "        # Predicción sobre held-out\n",
    "        if X_heldout is not None:\n",
    "            scaler.fit(X_heldout)\n",
    "            X_heldout_scaled = scaler.transform(X_heldout)\n",
    "            y_pred_heldout = best_model.predict(X_heldout_scaled)\n",
    "            gap_heldout = y_pred_heldout - y_heldout\n",
    "            gap_heldout_corrected = gap_heldout - (slope * y_heldout + intercept)\n",
    "            y_pred_corrected_heldout = y_heldout + gap_heldout_corrected\n",
    "\n",
    "            temp_heldout_df = pd.DataFrame({\n",
    "                'ID': X_heldout.index,\n",
    "                'y_labels': y_heldout,\n",
    "                'y_pred': y_pred_heldout,\n",
    "                'GAP': gap_heldout,\n",
    "                'GAP_corrected': gap_heldout_corrected,\n",
    "                'y_pred_corrected': y_pred_corrected_heldout\n",
    "            })\n",
    "\n",
    "            heldout_all_df = pd.concat([heldout_all_df, temp_heldout_df], ignore_index=True)\n",
    "\n",
    "    # Métricas finales\n",
    "    y_labels = np.array(y_labels)\n",
    "    y_predicts = np.array(y_predicts)\n",
    "\n",
    "    r_squared = r2_score(y_labels, y_predicts)\n",
    "    r_squared_ = np.mean(r_squared_l)\n",
    "    n, p = len(y_labels), X.shape[1]\n",
    "    r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "    mse = np.mean(mse_l)\n",
    "    rmse = np.mean(rmse_l)\n",
    "    mae = mean_absolute_error(y_labels, y_predicts)\n",
    "    F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "    p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "    F2 = r_squared / (1 - r_squared)\n",
    "\n",
    "    coef_df = pd.DataFrame(index=['_intercept'] + lista_vars,\n",
    "                           columns=['Estimate mean', 'Estimate std', 't value', 'p value'])\n",
    "\n",
    "    coef_df['Estimate mean'] = coef_array.mean(axis=1)\n",
    "    coef_df['Estimate std'] = coef_array.std(axis=1)\n",
    "\n",
    "    coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "    coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "    coef_df.loc['_intercept', 'R2 [+-]'] = np.std(r_squared_l)\n",
    "    coef_df.loc['_intercept', 'F2'] = F2\n",
    "    coef_df.loc['_intercept', 'mse'] = mse\n",
    "    coef_df.loc['_intercept', 'mse [+-]'] = np.std(mse_l)\n",
    "    coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "    coef_df.loc['_intercept', 'rmse [+-]'] = np.std(rmse_l)\n",
    "    coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "    coef_df.loc['_intercept', 'F'] = F\n",
    "    coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "    coef_df.loc['_intercept', 'MDE'] = mean_directional_accuracy_heldout(y_labels, y_predicts)\n",
    "    coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "    results_labels_df['y_pred_corrected'] = results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID', 'y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "\n",
    "    if shaps_comp:\n",
    "        explainer = shap.Explainer(best_model, X)\n",
    "        return [coef_df, r_squared_, results_labels_df, heldout_all_df, explainer, outer_cv.split(X)]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df, heldout_all_df, outer_cv.split(X)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7318f8-191e-4f22-ad6f-f33cda6dd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression_GBR_nested_heldout_mean_hyp(X, y, X_heldout=None, y_heldout=None, outer_splits=10, inner_splits=5, shaps_comp=False):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "        'max_depth': Integer(1, 5),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "    }\n",
    "\n",
    "    y_labels, y_predicts = [], []\n",
    "    r_squared_l, mse_l, rmse_l = [], [], []\n",
    "\n",
    "    results_labels_df = pd.DataFrame(columns=['y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'ID'])\n",
    "    coef_array = np.zeros([X.shape[1] + 1, outer_splits])\n",
    "    lista_vars = list(X.columns)\n",
    "\n",
    "    # Guardar los mejores parámetros por fold\n",
    "    best_params_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        print(f\"Fold: {fold}\")\n",
    "        \n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "        inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=model,\n",
    "            search_spaces=param_grid,\n",
    "            n_iter=3,\n",
    "            scoring='r2',\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        best_model = bayes_search.best_estimator_\n",
    "\n",
    "        # Guardar mejores parámetros\n",
    "        best_params_list.append(bayes_search.best_params_)\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        gap_test = y_pred - y_test\n",
    "        gap_train = best_model.predict(X_train) - y_train\n",
    "\n",
    "        slope, intercept, _, _, _ = linregress(y_train, gap_train)\n",
    "        corrected_gap = gap_test - (slope * y_test + intercept)\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            'y_labels': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'GAP': gap_test,\n",
    "            'GAP_corrected': corrected_gap,\n",
    "            'ID': X_test.index\n",
    "        })\n",
    "        results_labels_df = pd.concat([results_labels_df, temp_df], ignore_index=True)\n",
    "\n",
    "        y_labels.extend(y_test)\n",
    "        y_predicts.extend(y_pred)\n",
    "\n",
    "        r_squared_l.append(r2_score(y_test, y_pred))\n",
    "        mse_l.append(mean_squared_error(y_test, y_pred))\n",
    "        rmse_l.append(math.sqrt(mse_l[-1]))\n",
    "\n",
    "        coef_array[0, fold] = np.nan\n",
    "        coef_array[1:, fold] = best_model.feature_importances_\n",
    "\n",
    "    # --- Promediar hiperparámetros ---\n",
    "    def average_params(params_list):\n",
    "        df = pd.DataFrame(params_list)\n",
    "        averaged = {}\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'float' or df[col].dtype == 'int':\n",
    "                averaged[col] = df[col].mean()\n",
    "            else:\n",
    "                averaged[col] = df[col].mode()[0]\n",
    "        return averaged\n",
    "\n",
    "    avg_params = average_params(best_params_list)\n",
    "\n",
    "    # --- Entrenar modelo final con todo X y predecir heldout ---\n",
    "    heldout_all_df = pd.DataFrame(columns=['ID', 'y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected'])\n",
    "\n",
    "    if X_heldout is not None:\n",
    "        scaler.fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_heldout_scaled = scaler.transform(X_heldout)\n",
    "\n",
    "        final_model = GradientBoostingRegressor(random_state=42, **avg_params)\n",
    "        final_model.fit(X_scaled, y)\n",
    "\n",
    "        y_pred_heldout = final_model.predict(X_heldout_scaled)\n",
    "        gap_heldout = y_pred_heldout - y_heldout\n",
    "\n",
    "        gap_train_full = final_model.predict(X_scaled) - y\n",
    "        slope, intercept, _, _, _ = linregress(y, gap_train_full)\n",
    "        gap_heldout_corrected = gap_heldout - (slope * y_heldout + intercept)\n",
    "        y_pred_corrected_heldout = y_heldout + gap_heldout_corrected\n",
    "\n",
    "        heldout_all_df = pd.DataFrame({\n",
    "            'ID': X_heldout.index,\n",
    "            'y_labels': y_heldout,\n",
    "            'y_pred': y_pred_heldout,\n",
    "            'GAP': gap_heldout,\n",
    "            'GAP_corrected': gap_heldout_corrected,\n",
    "            'y_pred_corrected': y_pred_corrected_heldout\n",
    "        })\n",
    "\n",
    "    # --- Métricas finales ---\n",
    "    y_labels = np.array(y_labels)\n",
    "    y_predicts = np.array(y_predicts)\n",
    "\n",
    "    r_squared = r2_score(y_labels, y_predicts)\n",
    "    r_squared_ = np.mean(r_squared_l)\n",
    "    n, p = len(y_labels), X.shape[1]\n",
    "    r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "    mse = np.mean(mse_l)\n",
    "    rmse = np.mean(rmse_l)\n",
    "    mae = mean_absolute_error(y_labels, y_predicts)\n",
    "    F = (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "    p_value = np.round(scipy.stats.f.sf(F, n, (n - p - 1)), 15)\n",
    "    F2 = r_squared / (1 - r_squared)\n",
    "\n",
    "    coef_df = pd.DataFrame(index=['_intercept'] + lista_vars,\n",
    "                           columns=['Estimate mean', 'Estimate std', 't value', 'p value'])\n",
    "\n",
    "    coef_df['Estimate mean'] = coef_array.mean(axis=1)\n",
    "    coef_df['Estimate std'] = coef_array.std(axis=1)\n",
    "\n",
    "    coef_df.loc['_intercept', 'R2'] = r_squared\n",
    "    coef_df.loc['_intercept', 'R2 adj'] = r_squared_adj\n",
    "    coef_df.loc['_intercept', 'R2 [+-]'] = np.std(r_squared_l)\n",
    "    coef_df.loc['_intercept', 'F2'] = F2\n",
    "    coef_df.loc['_intercept', 'mse'] = mse\n",
    "    coef_df.loc['_intercept', 'mse [+-]'] = np.std(mse_l)\n",
    "    coef_df.loc['_intercept', 'rmse'] = rmse\n",
    "    coef_df.loc['_intercept', 'rmse [+-]'] = np.std(rmse_l)\n",
    "    coef_df.loc['_intercept', 'outcome var'] = np.var(y)\n",
    "    coef_df.loc['_intercept', 'F'] = F\n",
    "    coef_df.loc['_intercept', 'F-p_value'] = p_value\n",
    "    coef_df.loc['_intercept', 'MDE'] = mean_directional_accuracy_heldout(y_labels, y_predicts)\n",
    "    coef_df.loc['_intercept', 'MAE'] = mae\n",
    "\n",
    "    results_labels_df['y_pred_corrected'] = results_labels_df['y_labels'] + results_labels_df['GAP_corrected']\n",
    "    results_labels_df = results_labels_df[['ID', 'y_labels', 'y_pred', 'GAP', 'GAP_corrected', 'y_pred_corrected']]\n",
    "\n",
    "    if shaps_comp:\n",
    "        explainer = shap.Explainer(final_model, X)\n",
    "        return [coef_df, r_squared_, results_labels_df, heldout_all_df, explainer, outer_cv.split(X)]\n",
    "    else:\n",
    "        return [coef_df, r_squared_, results_labels_df, heldout_all_df, outer_cv.split(X)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3358e4-2a3a-4eb6-88eb-4dfb93d92c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143624bd-8db9-47ae-aa7e-a206d3bb31d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36bd6208-8a37-42c1-bbe5-4a35069a275e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd47e31-2cf7-42a0-8a37-77adef32af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0886f6f-ef46-4536-baf2-498d524e6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_list = ['Sex_1F_2M',\n",
    "              'Education',\n",
    "              'Barthel',\n",
    "              'Diabetes_1Y_0N',\n",
    "              'Hypertension_1Y_0N',\n",
    "              'Heart_Disease_1Y_0N',\n",
    "              'Physical_activity_1Y_0N',\n",
    "              'Cognition',\n",
    "              'Well_being_domain',\n",
    "              'Sleep_problems_1Y_0N',\n",
    "              'Audition_problems', \n",
    "              'Vision_problems']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46725d6-45f2-49e3-ae02-b24745180320",
   "metadata": {},
   "source": [
    "## Nested LOSO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bdfc0-6dc2-45a9-8468-bf45b2851785",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[vars_list]\n",
    "y1 = data['Age']\n",
    "groups = data['country']\n",
    "\n",
    "[coef_df_nested_loso, r_squared_nested_loso, results_labels_df_nested_loso, outer_cv_split_nested_loso] = Regression_GBR_leave_one_site_out(X1, y1, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d3747-d6f5-4306-b6d2-443e7c163f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
